[[install-hardware-requirements]]
= Hardware Requirements

ifeval::[{suma-content} == true]
This table outlines hardware and software requirements for the {productname} Server and Proxy, on {x86_64}, {arm}, {ppc64le} and {s390x} architecture.
endif::[]

ifeval::[{uyuni-content} == true]
This table outlines hardware and software requirements for the {productname} Server and Proxy on {x86_64}, and {arm} architecture.
endif::[]

//ifeval::[{suma-content} == true]
//For {ibmz} hardware requirements, see xref:installation-and-upgrade:install-ibmz.adoc[].
//endif::[]

For {productname} {smr} hardware requirements, see xref:retail:retail-requirements.adoc[].



[[server-hardware-requirements]]
== Server Requirements

{sle-micro} {microversion} is ths operation system of the container host and requires as free disk space:

* Minimum for base installation 100 GB
* Plus a minimum of 130 GB for repository data

By default the {productname} Server container stores mirrored repository (packages or products), database, and other data in subdirectories of the [path]``/var/lib/containers/storage/volumes/`` directory.
Repository synchronization fails if this directory runs out of disk space.
Estimate how much space the [path]``/var/lib/containers/storage/volumes/`` directory requires based on the number and kind of clients and repositories you plan to mirror.

For more information about filesystem and partitioning details, see xref:installation-and-upgrade:hardware-requirements.adoc#install-hardware-requirements-storage[] and the detailed installation instructions in the Installation and Deployment sections of this guide.

[cols="1,3,2", options="header"]
.Server Hardware Requirements
|===

| Hardware
| Details
| Recommendation

| CPU
| {x86_64}, {arm}, {ppc64le}, or {s390x}
| Minimum 4 dedicated 64-bit CPU cores

| RAM
| Minimum
| 16 GB

|
| Recommended
| 32 GB

| Disk Space
| [path]``/`` (root directory)
| 20 GB

|
| [path]``/var/lib/containers/storage/volumes``
| Minimum 150 GB (depending on the number of products)

|
| [path]``/var/lib/containers/storage/volumes/var-pgsql``
| Minimum 50 GB
|===

// |
// | [path]``/var/lib/containers/storage/volumes/var-cache``
// | Minimum 10 GB.
// Add 100 MB per {suse} product, 1 GB per {redhat} or other product.
// Consider to double the space if the server is used for Inter-Server Synchronization (ISS)
// 
// |
// | [path]``/var/lib/containers/storage/volumes/srv-www``
// | Minimum 100 GB
// 
// * Storage requirments should be calculated for the number of ISO distribution images, containers, and bootstrap repositories you will use.


The images by default have a 20 GB [literal]``/`` partition.
The cloud image of {sle-micro} 5.5 has just a 5 GB [literal]``/`` partition. Both work flawlessly with {productname}.
As long as external storage is mounted to [path]``/var/lib/containers/storage/volumes``, {productname} does not need or use storage on the [literal]``/`` partition, but leaves that to the management of the container host itself.



////
// When uncommenting verify the details! (POWER)

ifeval::[{suma-content} == true]
[cols="1,3,2", options="header"]
.Server Hardware Requirements for IBM POWER8 or higher processor–based server in Little Endian mode (ppc64le)
|===

| Hardware
| Details
| Recommendation

| CPU
|
| Minimum 4 dedicated cores

| RAM
| Minimum
| 16 GB

|
| Recommended
| 32 GB

| Disk Space
| [path]``/`` (root directory)
| Minimum 100 GB

|
| [path]``/var/lib/containers/storage/volumes/var-pgsql``
| Minimum 50 GB

|
| [path]``/var/lib/containers/storage/volumes/var-spacewalk``
| Minimum storage required: 100 GB (this will be verified by the implemented check)

* 50 GB for each {suse} product and Package Hub

* 360 GB for each {redhat} product

|
| [path]``/var/lib/containers/storage/volumes/var-cache``
| Minimum 10{nbsp}GB.
Add 100{nbsp}MB per {suse} product, 1{nbsp}GB per {redhat} or other product.
Double the space if the server is an ISS Master.

|
| [path]``/var/lib/containers/storage/volumes/srv-www``
| Minimum 100 GB

* Storage requirements should be calculated for the number of ISO distribution images, containers, and bootstrap repositories you will use.

|
| Swap space
| 3{nbsp}GB

|===

endif::[]

// end POWER
////



[IMPORTANT]
====
{productname} performance depends on hardware resources, network bandwidth, latency between clients and server, etc.

Based on the experience and different deployments that are in use, the advice for optimal performance of {productname} Server with an adequate number of proxies is to not exceed 10,000 clients per single server.
It is highly recommended to move to the Hub setup and involve consultancy when you have more than 10,000 clients.
Even with fine-tuning and an adequate number of proxies, such a large number of clients can lead to performance issues.

For more information about managing a large number of clients, see xref:specialized-guides:large-deployments/hub-multi-server.adoc[].
====


== Proxy Requirements

[cols="1,3,2", options="header"]
.Proxy Hardware Requirements
|===

| Hardware
| Details
| Recommendation

| CPU
| {x86_64}, {arm}
| Minimum 2 dedicated 64-bit CPU cores

| RAM
| Minimum
| 2 GB

|
| Recommended
| 8 GB

| Disk Space
| [path]``/`` (root directory)
| Minimum 40 GB

|
| [path]``/var/lib/containers/storage/volumes``
| Minimum 150 GB

|===

// /srv-www
// * Storage requirments should be calculated for the number of ISO distribution images, containers, and bootstrap repositories you will use.

// |
// | [path]``/var/lib/containers/storage/volumes/var-cache`` (Squid)
// | Minimum 100 GB

By default the {productname} Proxy container caches packages in the [path]``/var/lib/containers/storage/volumes/uyuni-proxy-squid-cache/`` directory.
If there is not enough space available, the proxy will remove old, unused packages and replace them with newer packages.

As a result of this behavior:

* The larger [path]``/var/lib/containers/storage/volumes/var-cache/`` directory is on the proxy, the less traffic will be between the proxy and the {productname} Server.
* By making the [path]``/var/lib/containers/storage/volumes/var-cache/`` directory on the proxy the same size as [path]``/var/lib/containers/storage/volumes/var-spacewalk/`` on the {productname} Server, you avoid a large amount of traffic after the first synchronization.
* The [path]``/var/lib/containers/storage/volumes/var-cache/`` directory can be small on the {productname} Server compared to the proxy.
  For a guide to size estimation, see the <<server-hardware-requirements>> section.



[[installation-postgresql-requirements]]
== Database Requirement

{postgresql} is the only supported database.
Using a remote {postgresql} database or remote file systems (such as NFS) with the {postgresql} database is not supported.
In other words, {postgresql} should be on the fastest available storage device for {productname}.

[IMPORTANT]
====
Because of potential performance issues, running a {postgresql} database remotely from {productname} is discouraged.
While such an environment is possible and even stable in many cases, there is always a risk of data loss if something goes wrong.

ifeval::[{suma-content} == true]
{suse} might not be able to provide assistance in such cases.
endif::[]
====



[[install-hardware-requirements-storage]]
== Persistent Storage and Permissions

Persistent volumes are created by default when deploying the container.

However, it is recommended that the volumes are stored on one or more separate storage devices.
Such a setup helps avoid data loss in production environments.
This can be done after container deployment.

Storage devices best should be set up after first deploying the container.
For more details, see xref:installation-and-upgrade:container-management/persistent-container-volumes.adoc[Persistent Container Volumes].

We recommend you use XFS as the filesystem type for all volumes.
The size of the disk for repositories storage is dependent on the number of distributions and channels you intend to manage with {productname}.
See the tables in this section for guides to estimate the size required.

On the {productname} Server, use this command to find all available storage devices:

----
hwinfo --disk | grep -E "Device File:"
----

Use the [command]``lsblk`` command to see the name and size of each device.

Use the [command]``mgr-storage-server`` command with the device names to set up the external disks as the locations for the storage and, optionally on a disk of its own, for the database:

----
mgr-storage-server <storage-disk-device> [<database-disk-device>]
----

The external storage volumes are set up as XFS partitions mounted at [path]``/manager_storage`` and [path]``/pgsql_storage``.

It is possible to use the same storage device for both channel data and the database.
This is not recommended, as growing channel repositories might fill up the storage, which poses a risk to database integrity.
Using separate storage devices may also increase performance.
If you want to use a single storage device, run [command]``mgr-storage-server`` with a single device name parameter.

If you are installing a proxy, the [command]``mgr-storage-proxy`` command takes only one device name parameter and will set up the external storage location as the Squid cache.



== Logical Volume Management (LVM)


// container-deployment/suma/server-deployment-vm-suma.adoc etc.
For all kind of virtual machines (VM), LVM is generally not needed and not recommended.
The disk setup is virtual and separate disks for volumes are possible and recommended.

// container-deployment/suma/server-deployment-suma.adoc
For other deployments, separate disks for volumes are also recommended.

On the {productname} Server, the [command]``mgr-storage-server`` command moves the complete content of the [path]``/var/lib/containers/storage/volumes`` directory to a separate disk and remounts it to [path]``/var/lib/containers/storage/volumes``.
Optionally, if a second device name is specified, [command]``mgr-storage-server`` moves the content of the [path]``/var/lib/containers/storage/volumes/var-pgsql`` database directory to a second separate disk and remounts it to [path]``/var/lib/containers/storage/volumes/var-pgsql``.

Similarily on the {productname} Proxy, the [command]``mgr-storage-proxy`` command moves the complete content of the [path]``/var/lib/containers/storage/volumes`` directory to a separate disk and remounts it to [path]``/var/lib/containers/storage/volumes``.




////
// *** The following is commented, and can probably deleted ***

// When installation takes place on bare metal (on-premise), [command]``cockpit`` can be used to create an LVM setup.
// In such a case, the disk setup needs to be performed manually.

// With containers {productname} now has [path]``/var/lib/containers/storage/volumes`` as an extra disk.
////
