[[vhm-caasp]]
= VHM and {caasp}

You can use a {productname} VHM to manage {caasp} clusters.
The VHM allows {productname} to obtain and report information about your clusters.
For more information on VHMs, see xref:client-configuration:vhm.adoc[].


[NOTE]
====
You can also manage {caasp} clusters directly with {productname}, without using a VHM.
For more information, see xref:client-configuration:virt-clusters.adoc[].
====



== Onboarding CaaSP Nodes

You can register each {caasp} node to {productname} using the same method as you would a Salt client.
For more information, see xref:client-configuration:registration-overview.adoc[].

We recommend that you create an activation key to associate {caasp} channels, and to onboard the associated nodes.
For more on activation keys, see xref:client-configuration:activation-keys.adoc[].

If you are using ``cloud-init``, we recommended that you use a bootstrap script in the ``cloud-init`` configuration.
For more on bootstrapping, see xref:client-configuration:registration-bootstrap.adoc[].

When you have added the {caasp} nodes to {productname}, the registered system will automatically apply the system lock Salt formula to prevent unintended actions on the client.
When a system is locked, the {webui} shows a warning and you can schedule actions using the {webui} or the API, but the action will fail.
For more information about system locks, see xref:client-configuration:system-locking.adoc[].

You can disable the System Lock formula from being automatically applied by editing the configuration file.
Open [path]``/etc/rhn/rhn.conf`` and add this line at the end of the file:

Add this line at the end of the [path]``/etc/rhn/rhn.conf`` file:

----
java.automatic_system_lock_cluster_nodes = false
----

Restart the spacewalk service to pick up the changes:

----
spacewalk-service restart
----

Updates related to {k8s} are managed using the ``skuba-update`` tool.
For more information, see https://documentation.suse.com/suse-caasp/4/html/caasp-admin.


[WARNING]
====
When using Salt or {productname} (either via UI or API) on any {caasp} nodes:

* Do not apply a patch (if the patch is marked as interactive)
* Do not mark a system to automatically install patches
* Do not perform an SP migration
* Do not reboot a node
* Do not issue any power management action via Cobbler
* Do not install a package if it breaks or conflicts the `patterns-caasp-Node-x.y`
* Do not remove a package if it breaks or conflicts or is one of the packages related with the `patterns-caasp-Node-x.y`
* Do not upgrade a package if it breaks or conflicts or is one of the packages related with the `patterns-caasp-Node-x.y`

Issuing those operations could render your {caasp} cluster unusable.
{productname} will not stop you from issuing these operations if the system is not locked.
====

== Autoinstallation Profile of a {caasp}{nbsp}4 Node

{caasp}{nbsp}4 provides an AutoYaST profile that you can use to autoinstall a node.
The profile is in the ``patterns-caasp-Management`` package.
For more information about the profile, see https://documentation.suse.com/suse-caasp/4.2/single-html/caasp-deployment/#_autoyast_preparation.

For an example script based based on the {caasp}{nbsp}4 template, customized to make use of {productname}, see https://github.com/SUSE/manager-build-profiles/tree/master/AutoYaST/CaaSP-autoinstall.

== Manage a {caasp} Cluster With {productname}

You can use {productname} to manage one or more existing {caasp} clusters.

[NOTE]
====
Only {caasp}{nbsp}4 is currently supported.
====


Before you begin, ensure you have installed your {caasp} cluster.

=== Elect a Management Node

To manage a {caasp} cluster, you need to elect a client as the management node for the cluster.
The management node cannot be part of the cluster, and it must have the {caasp} channels associated with it before you begin.
You can use a single management node for multiple clusters, as long as the clusters are all of the same kind.



.Procedure: Electing a Management Node
. In the {productname} {webui}, navigate to menu:Systems[System List] and click the name of the client to elect as the management node.
. Navigate to the menu:Formulas[Configuration] tab, and check the ``CaaSP Management Node`` formula.
. Click btn:[Save] and apply the highstate.


[NOTE]
====
You will not be able to use the management node until the highstate has been completed.
====


List all known clusters by navigating to menu:Clusters[Overview].
This list displays all existing clusters, along with the cluster type, and which management node they are associated with.
It also shows the nodes within the cluster, if the nodes are registered to {productname}.
For the nodes within a cluster, additional information from ``skuba`` and the {k8s} API are shown, including the role, status, and whether any updates are available.

For more information about the data available for nodes, see https://documentation.suse.com/suse-caasp/4/html/caasp-admin/_cluster_updates.html.

You will need to prepare the configuration from your cluster to the management node:

. Copy the ``skuba`` configuration directory from your cluster to the management node.
// Default file location? --LKB 2020-06-04
    This is the directory that the ``skuba`` service creates after the cluster has been bootstrapped. Take a note of the new file location for adding the cluster in the {productname} {webui}.

. Provide a way to authentication. There are two ways you can achieve this, choose the method that best suits your environment:
  * Copy the passwordless private SSH key used to access the cluster nodes to the {productname} Server, and take a note of the file location.
You need the current keys, and keys for any clients that you want to use in the future.
  * You can use an ``ssh-agent`` socket, and provide the path to the socket when setting up the cluster. There are two ways of using the ``ssh-agent`` with {caasp}:

    ** By using ``ssh-agent`` locally:
    *** Start the ssh-agent locally: ``eval $(ssh-agent)``
    *** Add the SSH key: ``ssh-add <key>``
    *** The socket used to access the agent is available in the ``$SSH_AGENT`` environment variable.

    ** Forward the `ssh-agent` to the management node from another machine:

      *** From your source machine: ``ssh -A <management node>``. The socket path is also available in the ``$SSH_AGENT`` environment variable.

[NOTE]
====
If you are using the ``ssh-agent`` method, the path of the socket changes every time a new ``ssh-agent``` is started or a new ``ssh -A`` connection is started.
The ``ssh-agent`` socket path can be updated at any time from the {productname} {webui}.
The socket path can also be overridden when starting any cluster action that requires SSH access.
====


=== Manage Clusters

To manage a cluster in {productname}, add the cluster in the {webui}.



.Procedure: Adding an Existing Cluster
. In the {productname} {webui}, navigate to menu:Clusters[Overview] and click btn:[FIXME].
. Follow the prompts to provide information about your cluster, including the cluster type, and select the management node to associate.
. Type the path to the ``skuba`` configuration file for the cluster.
// For example?
. Type the passwordless SSH key you want to use, or to the ``ssh-agent`` socket.
. Type a name, label, and description for the cluster.
. Click btn:[FIXME].


For each cluster you manage with {productname}, a corresponding system group is created.
By default, the system group is called ``Cluster <cluster_name>``.
Refresh the system group to update the list of nodes.
Only nodes known to {productname} are shown.


You can remove clusters from {productname} by navigating to menu:Clusters[Overview], unchecking the cluster to be deleted, and clicking btn:[Delete Cluster].


[IMPORTANT]
====
Deleting a cluster removes the cluster from {productname}, it does not delete the cluster nodes.
Workloads running on the cluster will continue uninterrupted.
====



=== Manage Nodes

When you have the cluster created in {productname}, you can manage nodes within the cluster.

Before you add a new node to the cluster, check the management node can access the node you want to add using passwordless SSH, or the ``ssh-agent`` socket you are forwarding.

You also need to ensure that the node you want to add is registered to {productname}, and has a {caasp} channel assigned.


.Procedure: Adding Nodes to a Cluster
. In the {productname} {webui}, navigate to menu:Clusters[Overview] and click btn:[Join Node].
. Select the nodes to add from the list of available nodes.
    The list of available nodes includes only nodes that are registered to {productname}, are not management nodes, and are not currently part of any cluster.
. Follow the prompts to enter the {caasp} parameters for the nodes to be added.
. OPTIONAL: Specify a custom ``ssh-agent`` socket that is valid only for the nodes that are being added.
. Click btn:[Save] to schedule an action to add the nodes.
    During this action, {productname} prepares the nodes for joining by disabling swap, then joins the nodes to the cluster.



.Procedure: Removing Nodes from a Cluster
. In the {productname} {webui}, navigate to menu:Clusters[Overview], uncheck the nodes to remove, and click btn:[Remove Node].
. Follow the prompts to define the parameters for the nodes to be removed.
. OPTIONAL: Specify a custom ``ssh-agent`` socket that is valid only for the nodes that are being removed.
. Click btn:[Save] to schedule an action to remove the nodes.

For more information about node removal, see https://documentation.suse.com/suse-caasp/4/single-html/caasp-admin/#_permanent_removal.



==== Upgrade the Cluster

If the cluster has available updates, you can use {productname} to schedule and manage the upgrade.

{productname} upgrades all control planes first, and then upgrades the workers.
For more information, see https://documentation.suse.com/suse-caasp/4.2/single-html/caasp-admin/#_cluster_updates.


.Procedure: Upgrading the Cluster
. In the {productname} {webui}, navigate to menu:Clusters[Overview], and click the cluster to upgrade.
. OPTIONAL: The are no {caasp} parameters available for you to customize for upgrade.
    However, you can specify a custom ``ssh-agent`` socket that is valid only for the nodes that are being upgraded.
. Click btn:[Save] to schedule an action to upgrade the cluster.


[NOTE]
====
{productname} will only interact with ``skuba`` to upgrade the cluster.
Any other required action, such as configuration changes, are not issued by {productname}.
====


For more information about upgrading, see https://www.suse.com/releasenotes/x86_64/SUSE-CAASP/4.
